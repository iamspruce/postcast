Here is today's tech briefing. caret Nowadays, being able to present an idea, project, or achievement is a must-have skill. The ability to showcase and talk about your work can determine whether you‚Äôre getting that degree, funding, or approval. But while effective communication is important, it‚Äôs not a skill everyone possesses. It‚Äôs something you build through consistent practice. Therein lies the challenge: when practicing on our own, it‚Äôs easy to overlook composure, posture, and delivery, which are just as important as the speech itself. That‚Äôs where we need a coach. They‚Äôre a second pair of eyes and ears that takes note of crucial details and relays them to you while you present. Thanks to recent advancements in visual A I, you can now receive continuous and objective feedback at any time. Frameworks like Vision Agents allow you to connect powerful visual models seamlessly and build your desired A I-powered applications. In this article, we‚Äôll build a real-time public speaking and presentation coach powered by Vision Agents, which you can run on your PC or Mac to practice and improve your delivery. Table of Contents ‚ÄãWhat We‚Äôll Be Building Technical Prerequisites Key Technologies Project Setup Set Up YOLO Coaching Instructions The Presentation Agent Conclusion ‚ÄãWhat We‚Äôll Be Building In this guide, we‚Äôll be walking through how to build a coaching agent that acts as your personal practice companion. This agent will provide real-time feedback, highlighting areas for improvement and offering helpful tips via audio and text. They‚Äôll track several aspects of your presentation, looking out for: Filler words: to help you reduce the use of words such as ‚Äúum‚Äù, ‚Äúuh‚Äù, ‚Äúlike‚Äù and ‚Äúyou know‚Äù. Speaking pace: to identify whether you‚Äôre talking too fast or too slow. Vocal variety: to point out if you‚Äôre sounding monotonous. Clarity: to listen to whether your words are clear enough. Posture: to check if you‚Äôre maintaining good or bad body posture. Look out for your shoulders, back and chin. Hand gestures: to monitor the use of your hands. Eye contact: to track whether your eyes are directly looking at your audience. You now have a mental picture of what we‚Äôre setting out to build. Better still, here‚Äôs a visual look at how this coach looks and works. You can find all the code in this tutorial in this repo. Technical Prerequisites Before we begin, ensure you have: A free Stream account. Python installed on your PC. An OpenAI A P I Key. Basic knowledge of Python. Key Technologies First, let's introduce the major players in our presentation coach implementation and their respective roles. Stream Video Stream Video is a complete video infrastructure built on WebRTC, enabling browsers and apps to send live audio and video. It comes supercharged with a global edge network that routes your video to the closest server in under 30 milliseconds. This means that for our presentation coach, the A I can join your practice session like a real participant, seeing and hearing you in real-time with no lag while also providing feedback. ‚ÄãVision Agents Vision Agents is an open-source framework from Stream that allows you to connect video streams, A I models, and chat interfaces. It ships with Stream Video as its default transport layer. This framework simplifies the development of multimodal A I agentic applications by providing a unified Agent class that orchestrates everything. With Vision Agents, you can connect models and also get them to work together seamlessly as one coordinated system. OpenAI RealTime A P I OpenAI RealTime A P I allows you to stream live, low-latency interactions with OpenAI models. Its strength lies in its ability to handle speech-to-speech in one go. Your words go in, the A I thinks about them, and you get audio and text feedback almost instantaneously. Your app and model can communicate instantly, just like a live conversation. This will be the presentation coach‚Äôs actual brainbox. YOLO11 YOLO11 is a modern and powerful computer vision model developed by Ultralytics. It supports a wide range of tasks, including object detection, instance segmentation, image classification, pose estimation slash keypoint detection, and oriented bounding box detection. It tracks 17 different points on your body, such as your shoulders, head, and hand positions, and also attempts to determine your posture at specific times. Our presentation coach will focus on the aspects of pose estimation and keypoint detection. Project Setup Now, let's get straight into building this presentation coach with all the technologies we‚Äôve highlighted. We‚Äôll start bu installing uv, which is the recommended installer for Vision Agents. Create a project folder and run this command in the terminal if you‚Äôre using pip installer: pip install uv For Linux slash MacOS, run: curl -LsSf Link in description. pipe sh. For Windows, run: powershell -ExecutionPolicy ByPass -c \"irm Link in description. pipe iex\" Next, initialize uv in your project: uv init Then create a virtual environment: uv venv And activate the virtual environment: .venv\Scripts\activate Now install Vision Agents with the required plugins and dependencies: uv add vision-agents[getstream,openai,ultralytics] python-dotenv In the root directory, create an .env file and provide the necessary credentials: STREAM underscore A P I underscore KEY=your-stream-api-key STREAM underscore A P I underscore SECRET=your-stream-secret OPENAI underscore A P I underscore KEY=your-openai-api-key CALL underscore ID="practice-room" slash slash feel free to call it any name In the root directory, create an 'instructions' folder and a Markdown file called 'coach.md' inside it. In the root directory, create a file and name it download underscore yolo underscore pose.py. Your current project folder structure should look like this: ‚îî‚îÄ‚îÄ üìÅPresentation Coach ‚îî‚îÄ‚îÄ üìÅ.venv ‚îî‚îÄ‚îÄ üìÅinstructions ‚îú‚îÄ‚îÄcoach.md ‚îî‚îÄ‚îÄ .env ‚îî‚îÄ‚îÄ .gitignore ‚îî‚îÄ‚îÄ download underscore yolo underscore pose.py ‚îî‚îÄ‚îÄ main.py ‚îî‚îÄ‚îÄ pyproject.toml ‚îî‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ uv.lock Set Up YOLO The Ultralytics YOLO11 framework uses the yolo11n-pose.pt model file to watch your posture throughout your presentation. This pre-trained deep learning model file performs pose estimation by detecting keypoints. In your download underscore yolo underscore pose.py file, insert this: slash slash download underscore yolo underscore pose.py from ultralytics import YOLO import shutil from pathlib import Path model = YOLO("yolo11n-pose.pt") project underscore root = Path( underscore underscore file underscore underscore ).parent target = project underscore root slash "yolo11n-pose.pt" if not target.exists(): print("Copying model to project root...") shutil.copy2(model.model.path, target) else: print("Model already in project root.") print(f"Ready: {target.resolve()}") This automatically downloads the yolo11n-pose.pt file, if absent in your project, and copies it to the project root. Coaching Instructions A I plays the role of the coach in this implementation. The coach.md file gives it its entire personality, expertise, and coaching philosophy. You specify the tone, output rate, length of response, speaking pace, timing of feedback, and other metrics you want your A I to follow. Without these instructions, you‚Äôll receive generic responses, vague tips, lengthy responses, and interruptions. Paste this in your coach.md file to get the best results: slash slash instructions slash coach.md These instructions describe how the coaching system should behave when someone is practicing a presentation. Ensure to give quick, specific tips and try not to interrupt their flow. Only provide feedback after detecting at least 3-5 seconds of silence. On format, feedback should appear like short texts on screen and keep them within 1 or 2 sentences maximum. You want people to be relaxed during the presentation, so, ensure you start with something positive and always add one actionable tip. You‚Äôll have access to video feeds, transcripts and pose data. That‚Äôs enough to get a good idea of pace, body language and how engaged they look. A big part of your evaluation is to understand their speech. You should look out for: Pace: Shouldn‚Äôt be too fast or too slow. Send a message to address it when noticed. Filler words: Listen for ‚Äúum‚Äù,‚Äùuh‚Äù,‚Äùemm‚Äù,‚Äùyou know‚Äù. If they keep popping up, send a reminder to pause. Tone and variety: Watch out for their pitch and suggest accordingly Clarity: Make sure that their words are clear enough Also, keep an eye on the body posture. Encourage confident presentation involving use of hands, straight shoulders and steady eye contact. The Presentation Agent Now that we have all our pieces in place, it‚Äôs time to look at the Central Processing Unit of our presentation coach. Inside the main.py file is where the magic of Vision Agents happens, tying live video streaming, OpenAI Realtime functions, YOLO pose detection, and your coaching instructions into one multimodal agent. Here‚Äôs how our main.py looks: slash slash main.py import logging from dotenv import load underscore dotenv from vision underscore agents.core import Agent, User, cli from vision underscore agents.core.agents import AgentLauncher from vision underscore agents.plugins import getstream, openai, ultralytics load underscore dotenv() async def create underscore agent( star star kwargs) -> Agent: agent underscore user = User( name="Public Speaking and Presentation Coach", id="coach underscore agent", image="Link in description. ) return Agent( edge=getstream.Edge(), agent underscore user=agent underscore user, instructions=" at instructions slash coach.md", llm=openai.Realtime( fps=6, voice="alloy", ), processors=[ ultralytics.YOLOPoseProcessor(model underscore path="yolo11n-pose.pt") ], ) async def join underscore call(agent: Agent, call underscore type: str, call underscore id: str, star star kwargs) -> None: print(f"Presentation Coach Agent starting...") print(f"Joining call: {call underscore type}:{call underscore id}") call = await agent.create underscore call(call underscore type, call underscore id) session = await agent.join(call) print("Agent connected and ready!") print("Real-time coaching enabled") try: await agent.llm.simple underscore response( text="Greet the user warmly and say you're ready to help them practice. " "Watch their body language and speech ‚Äî give encouraging, real-time feedback." ) await agent.finish() finally: await session.close() if name == " underscore underscore main underscore underscore ": cli(AgentLauncher(create underscore agent=create underscore agent, join underscore call=join underscore call)) Let‚Äôs walk through what‚Äôs happening in this code: Your keys are loaded from the .env file by the load underscore dotenv function. The create underscore agent function then creates the coach‚Äôs identity using the User object, assigning it a name, ID, and avatar. The instantiated Agent object takes several arguments, which configure how the agent behaves and interacts with video, models, and the user. Our Agent object has the following arguments: edge, agent underscore user, instructions, LLMS, and processors. edge=getstream.Edge() connects everything to Stream‚Äôs global, low-latency video infrastructure. agent underscore user defines the coach‚Äôs identity created earlier. Instructions load your coaching philosophy located in the coach.md directly into the agent‚Äôs brain. llm specifies the A I language model and parameters. For this agent, it is OpenAI.Realtime, which opens a WebSocket to OpenAI‚Äôs Realtime A P I. With a frame rate of 6, the agent receives six video frames per second. The voice parameter set to ‚Äúalloy‚Äù allows for real-time speech generation. processors perform specific types of A I slash M L computation on incoming streams. In this case, video frames were analysed by YOLO11. With the join underscore call function, the agent joins the call with a short, welcoming greeting that appears instantly in the chat. The await agent.finish function hands control over to the real-time loop of the agent, which continuously listens, watches, thinks, and responds automatically. No need for manual prompts. To run the agent, enter this on your terminal: python main.py Conclusion We have successfully developed a public speaking and presentation A I agent that provides timely feedback with valuable tips to help you improve your presentation in real-time. This was made possible by the trio of Vision Agents, YOLO11, and OpenAI Realtime A P I. In under 50 lines of code, we were able to build an agent that costs nearly nothing (just a few tokens) compared to paying dollars 99 for a Sass platform or hiring a physical coach. Pretty cool. With Vision Agents, you have a developer-friendly framework that opens up numerous opportunities for builders to create engaging A I apps efficiently. Happy building! Learn to code for free. freeCodeCamp's open source curriculum has helped more than 40,000 people get jobs as developers. Get started

Thanks for listening. Find the full article link in the description.